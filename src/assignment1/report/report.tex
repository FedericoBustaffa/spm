\documentclass[12pt, a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{mathtools, amsmath, amssymb, amsthm}
\usepackage[hidelinks]{hyperref}
\usepackage{tabularx}
\usepackage{svg}
\usepackage{caption}
\usepackage{float}
\usepackage{booktabs}

\title{Assignment 1\\Intrinsics and Auto-Vectorization}
\author{Federico Bustaffa}
\date{12/03/2025}

\begin{document}

\maketitle
\tableofcontents
\clearpage

\section{Introduction}

This report discuss the methodology adopted for the optimized implementations
of the \textit{softmax} function. The first one using the \textit{auto-vectorization}
provided by the compiler and the second using explicit vectorization with
\textit{intrinsics}.

Both versions were compared to each other and with the \textit{plain} naive
version provided by the teacher, mainly evaluating the execution time as the
input size varies.

\section{Methods and Implementation}

The plain version of the algorithm has been compiled with only the \verb|-O2|
flag, disabling the \textit{auto-vectorization}; mainly to see the improvement
over a non vectorized version.

For both versions, optimizations were applied incrementally by adding changes
and testing performance at each step.

\subsection{Auto-Vectorization}

To check if optimizations were applied by the compiler, it has been used the
\verb|-fopt-info-vec-missed| flag.

The first step was to use \verb|-O3| and \verb|-march=native| flags to enable
all possible optimizations the compiler can apply. At this stage, the compiler
notifies that basically every loop can't be vectorized.

To let the compiler optimize the code, the first changes were:
\begin{itemize}
	\item Add the \verb|__restricted__| qualifier to \verb|input| and
	      \verb|output| arrays, in order to avoid pointer aliasing.
	\item Split the second loop in two loops, in order to let the first line
	      not be affected by the \textit{write-after-write} dependecy that the
	      second line causes.
	\item Compute the inverse of the sum before the last loop and perform a
	      multiplication instead of a division, generally less performant than
	      the first one.
\end{itemize}
The first step of the algorithm also finds the maximum value of the given array
by looping over it. To improve this step the \textit{loop unrolling} technique
was applied by a factor of 2 and 4.

In the end the \verb|-ffast-math| flag was enabled, causing the higher
improvement but also some approximation error.

\subsection{Intrinsics}

The \textit{intrinsics} version was also compiled with some optimization flag
like \verb|-O3|, \verb|-march=native| and \verb|-mavx|, to enable the vectorized
instruction set and improve memory access.

In order to find the max value of the array, the algorithm works on chunks of 8
elements at a time, comparing 2 chunks and returning a chunk where in position
$i$ there is the max value between $a_i$ and $b_i$, where $a$ and $b$ are the
two chunks. So a \verb|vmax| vector of 8 elements were updated at every
iteration, consuming all the elements in the array.

In the end, similarly to an horizontal sum, the absolute maximum were extracted
with some shift and \textit{sse} intrinsics operations.

In the first implementation, the comparisons were done with bitmasks and blend
operations, while in the final implementation the \verb|_mm256_max_ps| and
\verb|_mm_max_ps| functions are used for better performance.

The second loop was improved by computing the exponantial with the
\verb|exp256_ps| function provided and accumulating the sum in a 8 elements
vector. Once completed the loop an horizontal sum was computed to obtain the
final sum value. The final loop was changed in a vectorized division of a
vector by a scalar.

\section{Experiments and Comparisons}

Every test was repeated ten times, taking the average time of the total
computation of the function (no fine-grained benchmark was performed). Here
some results from the various implementations explored.

\begin{table}[H]
	\centering
	\begin{tabular}{r | rrrrr}
		\toprule
		Elements & Plain    & \verb|-O3| + Code & \verb|-ffast-math| & $\text{AVX}_\text{mask}$ & $\text{AVX}_\text{max}$ \\
		\midrule
		128      & 6.3744   & 6.1925            & 6.4468             & 1.2825                   & 0.8136                  \\
		1024     & 18.2587  & 16.7189           & 10.3464            & 6.3154                   & 4.6816                  \\
		8192     & 110.9500 & 89.1472           & 37.2496            & 43.3424                  & 29.2679                 \\
		16384    & 222.0010 & 173.7950          & 70.2605            & 83.8557                  & 55.7626                 \\
		\bottomrule
	\end{tabular}
	\caption{Execution time in $\mu$s for different implementations}
	\label{tab: times}
\end{table}

As reported in table \ref{tab: times}, the second AVX version was the best, in
terms of execution time and speed up for every test performed.

To see how much every version improve the plain one, the speed up values were
also computed and reported in the table below.

\begin{table}[H]
	\centering
	\begin{tabular}{r|rrrr}
		\toprule
		Elements & \verb|-O3| + Code & fast-math & $\text{AVX}_\text{mask}$ & $\text{AVX}_\text{max}$ \\
		\midrule
		128      & 1.0294            & 0.9888    & 4.9704                   & 7.8350                  \\
		1024     & 1.0921            & 1.7647    & 2.8912                   & 3.9001                  \\
		8192     & 1.2446            & 2.9786    & 2.5598                   & 3.7908                  \\
		16384    & 1.2774            & 3.1597    & 2.6474                   & 3.9812                  \\
		\bottomrule
	\end{tabular}
	\caption{Speed Up values for different implementations}
	\label{tab: speedup}
\end{table}

The plot below shows how both AVX versions work very well on small inputs and
stabilize around some speed up value as the input dimension increases.

\begin{figure}[H]
	\centering
	\includesvg[width=0.75\linewidth]{speed_up.svg}
	\caption{Speed Up trend over input dimension}
\end{figure}

For the auto version with \verb|-ffast-math| we can see instead how the speed
up increase as the input gets larger, surpassing one of the AVX versions for
large input sizes.

\section{Conclusions}

In conclusion, the AVX versions are better in most cases, even for large arrays;
they are also more verbose and difficult to implement, but at least give much
more control over code.

On the other hand, the auto versions are simpler to implement but require more
compiler work and to see a significant speed up, flags like \verb|-ffast-math|
must be enabled, causing also approximation errors.

\end{document}
